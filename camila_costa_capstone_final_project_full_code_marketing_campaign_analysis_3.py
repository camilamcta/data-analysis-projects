# -*- coding: utf-8 -*-
"""Camila_Costa_Capstone_Final_Project_Full_Code_Marketing_Campaign_Analysis-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mhBdCFYMRM3fvX50BMC1rmZzTMj35wM8

# **Marketing Campaign Analysis**

## **Problem Definition**

### **The Context:**

 - Why is this problem important to solve?
   Creating and Understanding consumer profile from a grocery store because it can help to improve sales by providing personalized marketing campaigns.

### **The objective:**

 - What is the intended goal?
    The intended goal is to segment clients to help the marketing and campaign team in providing segmented marketing campaigns.

### **The key questions:**

- What are the key questions that need to be answered?
    How can the consumers be segmented? What are the consumption patterns from consumers? Are there correlation between segments? Are there correlation between segments and campaign offer acceptance?

### **The problem formulation**:

- What is it that we are trying to solve using data science?
    Customers segmentation from a grocery store.

------------------------------
## **Data Dictionary**
------------------------------

The dataset contains the following features:

1. ID: Unique ID of each customer
2. Year_Birth: Customerâ€™s year of birth
3. Education: Customer's level of education
4. Marital_Status: Customer's marital status
5. Kidhome: Number of small children in customer's household
6. Teenhome: Number of teenagers in customer's household
7. Income: Customer's yearly household income in USD
8. Recency: Number of days since the last purchase
9. Dt_Customer: Date of customer's enrollment with the company
10. MntFishProducts: The amount spent on fish products in the last 2 years
11. MntMeatProducts: The amount spent on meat products in the last 2 years
12. MntFruits: The amount spent on fruits products in the last 2 years
13. MntSweetProducts: Amount spent on sweet products in the last 2 years
14. MntWines: The amount spent on wine products in the last 2 years
15. MntGoldProds: The amount spent on gold products in the last 2 years
16. NumDealsPurchases: Number of purchases made with discount
17. NumCatalogPurchases: Number of purchases made using a catalog (buying goods to be shipped through the mail)
18. NumStorePurchases: Number of purchases made directly in stores
19. NumWebPurchases: Number of purchases made through the company's website
20. NumWebVisitsMonth: Number of visits to the company's website in the last month
21. AcceptedCmp1: 1 if customer accepted the offer in the first campaign, 0 otherwise
22. AcceptedCmp2: 1 if customer accepted the offer in the second campaign, 0 otherwise
23. AcceptedCmp3: 1 if customer accepted the offer in the third campaign, 0 otherwise
24. AcceptedCmp4: 1 if customer accepted the offer in the fourth campaign, 0 otherwise
25. AcceptedCmp5: 1 if customer accepted the offer in the fifth campaign, 0 otherwise
26. Response: 1 if customer accepted the offer in the last campaign, 0 otherwise
27. Complain: 1 If the customer complained in the last 2 years, 0 otherwise

**Note:** You can assume that the data is collected in the year 2016.

## **Import the necessary libraries and load the data**
"""

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# To scale the data using z-score
from sklearn.preprocessing import StandardScaler

# To compute distances
from scipy.spatial.distance import cdist

# To perform K-means clustering and compute Silhouette scores
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# To visualize the elbow curve and Silhouette scores
from yellowbrick.cluster import SilhouetteVisualizer

# Importing PCA
from sklearn.decomposition import PCA

# To encode the variable
from sklearn.preprocessing import LabelEncoder

# Importing TSNE
from sklearn.manifold import TSNE

# To perform hierarchical clustering, compute cophenetic correlation, and create dendrograms
from sklearn.cluster import AgglomerativeClustering, DBSCAN
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet

# To compute distances
from scipy.spatial.distance import pdist

# To import K-Medoids
!pip install scikit-learn-extra
from sklearn_extra.cluster import KMedoids

# To import Gaussian Mixture
from sklearn.mixture import GaussianMixture

# To supress warnings
import warnings

warnings.filterwarnings("ignore")

# Import data drom drive
from google.colab import drive
drive.mount('/content/drive')
data = pd.read_csv('/content/drive/MyDrive/Python/MIT/final project/marketing_campaign+%284%292.csv')

"""## **Data Overview**

- Reading the dataset
- Understanding the shape of the dataset
- Checking the data types
- Checking for missing values
- Checking for duplicated values
- Drop the column which has no null values
"""

# Reading the dataset
data.head()

# Understanding the shape of the dataset
data.info()

"""Dataset presents 2240 entries and 27 columns. It looks like income has missing values, but we will check this further."""

# Checking the data types
data.dtypes

"""Education, Marital Status and DT_Customer are not numerical. The only float variable is Income."""

# Checking for missing values
missing_values = data.isnull().sum()
missing_values

"""It is confirmed that Income has missing values."""

# Checking for duplicated values
data.duplicated().sum()

"""# Cleaning data and dropping column

ID has no null values and it is equal to the number of observations. It means an index from data and it doesn't look like it will be usefull for predictions or analysis. That is why, it was dropped.


"""

# Drop the column ID
data.drop('ID', axis=1, inplace=True)

# Find the percentage of missing values in each column of the data after dropping ID

missing_percentage = (data.isnull().sum() / len(data)) * 100

# Display the result
print(missing_percentage)

"""#### **Observations and Insights from the Data overview:**
* The dataset has originally 2240 entries and 27 columns. After dropping ID, it has 2240 entries and 26 columns.
* 'Education', 'Marital Status' And Dt_Customer' are non-numerical.
* 'Income' is the only column with missing value. Is misses 24 values and it is around 1%.

## **Exploratory Data Analysis (EDA)**

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Questions:**

1. What is the summary statistics of the data? Explore summary statistics for numerical variables and the categorical variables
2. Find out number of unique observations in each category of categorical columns? Write your findings/observations/insights
3. Are all categories different from each other or can we combine some categories? Is 2n Cycle different from Master?
4. There are 8 categories in Marital_Status with some categories having very low count of less than 5. Can we combine these categories with other categories?
"""

# Summary Statistics
data.describe()

# Data Statistics for Categorical Variables
data.describe(include=['object'])

# Unique Observations
categorical_cols = data.select_dtypes(include=['object']).columns

for col in categorical_cols:
    print(f"Column: {col}")
    print(data[col].value_counts())
    print("-" * 20)

"""* Education: Graduation is the higher. PhD is the second, but Master and 2n Cycle are the same and they can be combined. The last one is basic.
* Marital Status: Alone, Absurd, YOLO (you only live once) have less than 50. Since it is not possible to define if they can be combined to other status like single, the option will be create a new category 'other status'.
"""

# Creating a new category to combine Alone, Absurd and YOLO
data['Marital_Status'] = data['Marital_Status'].replace(['Alone', 'Absurd', 'YOLO'], 'Other Status')

# Printing Marital_Status to check changes
print(data['Marital_Status'].value_counts())

# Combining master and 2n cycle
data['Education'] = data['Education'].replace(['Master', '2n Cycle'], 'Master')

# Printing Education to check changes
print(data['Education'].value_counts())

"""## Analysis
* It seems like Income has high-income outliers.
* Education: The most frequent education level is "Graduation," followed by "PhD" and "Master.". After combining Master and 2n Cycle, Master become the second one.
* A significant portion of customers have no kids or teens at home. It is 50%.
* Around 90% of customers didn't complain.
* After combining categories, the most frequent marital status is "Married," followed by "Together" and "Single". A new category was created: "Other Status". This new category was also small. Decided to add it to Single.
"""

data['Marital_Status'] = data['Marital_Status'].replace('Other Status', 'Single')

# Printing Marital_Status to check changes
print(data['Marital_Status'].value_counts())

"""### **Univariate Analysis on Numerical and Categorical data**

Univariate analysis is used to explore each variable in a data set, separately. It looks at the range of values, as well as the central tendency of the values. It can be done for both numerical and categorical variables.

- Plot histogram and box plot for different numerical features and understand how the data looks like.
- Explore the categorical variables like Education, Kidhome, Teenhome, Complain.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Leading Questions**:
1. How does the distribution of Income variable vary across the dataset?
2. The histogram and the box plot are showing some extreme value on the right side of the distribution of the 'Income' feature. Can we consider them as outliers and remove or should we analyze these extreme values?
3. There are only a few rows with extreme values for the Income variable. Is that enough information to treat (or not to treat) them? At what percentile the upper whisker lies?
"""

label_encoder = LabelEncoder()  # Create a LabelEncoder instance
for col in ['Education', 'Marital_Status']:
    data[col] = label_encoder.fit_transform(data[col])

# Plotting Histograms for Numerical Features
numerical_features = data.select_dtypes(include=['number']).columns
for feature in numerical_features:
    plt.figure(figsize=(8, 6))
    sns.histplot(data[feature], kde=True)
    plt.title(f'Histogram of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.show()

# Plotting Box Plots
for feature in numerical_features:
    plt.figure(figsize=(8, 6))  # Adjust figure size as needed
    sns.boxplot(data[feature])
    plt.title(f'Box Plot of {feature}')
    plt.xlabel(feature)
    plt.show()

"""Besides Income, there are also outliers in the products types MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds. However, it doesn't look like is necessary to remove them since they can provide information about customers profile.

## Income Analysis
* The Distribution of Income is right-skewed. Customers with highly incomes are pulling the tail to the right. It was possible to confirm the presence of outliers.
* The option will be remove these outliers.
"""

# Whisker's Percentile
upper_whisker = np.percentile(data['Income'].dropna(), 75) + 1.5 * (np.percentile(data['Income'].dropna(), 75) - np.percentile(data['Income'].dropna(), 25))
percentile = np.percentile(data['Income'].dropna(),  np.where(data['Income'].dropna() <= upper_whisker)[0][-1]/len(data['Income'].dropna())*100)
print(f"The upper whisker of the Income variable lies at approximately the {percentile:.2f}th percentile.")

# Removing the outliers from Income
Q1 = data['Income'].quantile(0.25)
Q3 = data['Income'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter data
filtered_data = data[(data['Income'] >= lower_bound) & (data['Income'] <= upper_bound)]

# Assuming 'filtered_data' contains the data after outlier removal
plt.figure(figsize=(8, 6))
sns.histplot(filtered_data['Income'], kde=True)
plt.title('Distribution of Income (After Outlier Removal)')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(filtered_data['Income'])
plt.title('Box Plot of Income (After Outlier Removal)')
plt.xlabel('Income')
plt.show()

"""## Actions

* Removed outliers from income.
* Added missing values to Income.

### **Bivariate Analysis**

- Analyze different categorical and numerical variables and check how different variables are related to each other.
 - Check the relationship of numerical variables with categorical variables.
"""

# Convert 'Dt_Customer' to datetime objects, specifying the correct format
data['Dt_Customer'] = pd.to_datetime(data['Dt_Customer'], format='%d-%m-%Y')

# Extract numerical features for correlation analysis
numerical_data = data.select_dtypes(include=['number'])

# Transforming Education and Marital Status to Numerical

data['Education'] = label_encoder.fit_transform(data['Education'])
data['Marital_Status'] = label_encoder.fit_transform(data['Marital_Status'])

plt.figure(figsize=(15, 7))
sns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")  # Plotting the correlation plot
plt.show()

"""* Higher education is generally correlated with higher income."""

# Relationship between Education and Income
sns.barplot(x='Education', y='Income', data=data)
plt.title('Average Income by Education Level')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels if needed
plt.show()

# Marital Status and Income
sns.barplot(x='Marital_Status', y='Income', data=data)
plt.title('Average Income by Marital Status')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels if needed
plt.show()

# Kids and Teen and Income
sns.barplot(x='Kidhome', y='Income', hue='Teenhome', data=data)
plt.title('Average Income by Number of Kids and Teens at Home')
plt.xlabel('Number of Kids at Home')
plt.ylabel('Average Income')
plt.legend(title='Number of Teens at Home')
plt.show()

"""### **Feature Engineering and Data Processing**

In this section, we will first prepare our dataset for analysis.
- Imputing missing values
"""

# Imputing missing values
mean_income = data['Income'].median()
data['Income'].fillna(mean_income, inplace=True)

"""**Think About It:**

- Can we extract the age of each customer and create a new feature?
- Can we find the total kids and teens in the home?
- Can we find out how many members each family has?
- Can we find the total amount spent by the customers on various products?
- Can we find out how long the customer has been with the company?
- Can we find out how many offers the customers have accepted?
- Can we find out amount spent per purchase?


"""

# Current year
current_year = 2016

data["Age"] = 2016 - pd.to_datetime(data["Dt_Customer"], format="%m/%d/%Y").dt.year

# Sorting the values in ascending order
data["Age"].sort_values()

# Display the updated DataFrame
print(data.head())

# Create the 'Age' feature
data['Age'] = current_year - data['Year_Birth']

# Checking if there are anyone older than 105
# Filter the DataFrame to find clients older than 105
older_than_105 = data[data['Age'] > 105]

# Print the filtered DataFrame
print(older_than_105)

# Print the number of clients older than 105
print(f"Number of clients older than 105: {len(older_than_105)}")

# Filter the DataFrame to keep rows where 'Age' is less than or equal to 105
data = data[data['Age'] <= 105]

# Reset the index
data = data.reset_index(drop=True)

# Total under 18 years old at home
data['Total_Kids'] = data['Kidhome'] + data['Teenhome']

# Display the updated DataFrame
print(data.head())

# Family Members

# Create a new column for estimated family size
data['Family_Size'] = 2 + data['Kidhome'] + data['Teenhome']  # Start with 2 for adults

# Adjust for single-person households
data.loc[data['Marital_Status'] == 'Single', 'Family_Size'] = 1 + data['Kidhome'] + data['Teenhome']

# Display the updated DataFrame
print(data.head())

# Total Amount Spent
data['Total_Spent'] = data['MntWines'] + data['MntFruits'] + data['MntMeatProducts'] + \
                      data['MntFishProducts'] + data['MntSweetProducts'] + data['MntGoldProds']

# Display the updated DataFrame
print(data.head())

# Client Antiguity

# Assume a reference date for calculating tenure (e.g., the most recent date in the data)
reference_date = data['Dt_Customer'].max()

# Calculate customer tenure in days
data['Customer_Tenure'] = (reference_date - data['Dt_Customer']).dt.days

# Display the updated DataFrame
print(data.head())

# Offers Accepted
data['Total_Offers_Accepted'] = data['AcceptedCmp1'] + data['AcceptedCmp2'] + data['AcceptedCmp3'] + \
                                 data['AcceptedCmp4'] + data['AcceptedCmp5'] + data['Response']

# Display the updated DataFrame
print(data.head())

#Amount Spent Per Purchase
# Calculate total number of purchases
data['Total_Purchases'] = data['NumDealsPurchases'] + data['NumCatalogPurchases'] + \
                           data['NumStorePurchases'] + data['NumWebPurchases']

# Calculate amount spent per purchase
data['Amount_per_Purchase'] = data['Total_Spent'] / data['Total_Purchases']

# Handle cases where Total_Purchases is 0 (to avoid division by zero errors)
data['Amount_per_Purchase'] = data['Amount_per_Purchase'].fillna(0)  # Replace NaN with 0

# Display the updated DataFrame
print(data.head())

# Plotting Income x Expenses
plt.figure(figsize=(20, 10))

sns.scatterplot(x="Income", y="Total_Spent", data=data)  # Using sns.scatterplot()

plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.xlabel("Income", fontsize=20, labelpad=20)
plt.ylabel("Expenses", fontsize=20, labelpad=20)
plt.show() # to display the plot

# Plotting Family Size x Income
sns.barplot(x='Family_Size', y='Income', data=data)
plt.title('Average Income by Family Size')
plt.xlabel('Family Size')
plt.ylabel('Average Income')
plt.show()

"""## **Important Insights from EDA and Data Preprocessing**
* Income is not evenly distributed, potentially with a right-skewed distribution (more customers in lower income brackets).

## **Data Preparation for Segmentation**

- The decision about which variables to use for clustering is a critically important decision that will have a big impact on the clustering solution. So we need to think carefully about the variables we will choose for clustering. Clearly, this is a step where a lot of contextual knowledge, creativity, and experimentation/iterations are needed.
- Moreover, we often use only a few of the data attributes for segmentation (the segmentation attributes) and use some of the remaining ones (the profiling attributes) only to profile the clusters. For example, in market research and market segmentation, we can use behavioral data for segmentation (to segment the customers based on their behavior like amount spent, units bought, etc.), and then use both demographic as well as behavioral data for profiling the segments found.
- Plot the correlation plot after we've removed the irrelevant variables
- Scale the Data

* Decided to drop the columns: Year_Birth, Dt_customer, complain, response, AcceptedCmp1,AcceptedCmp2, AcceptedCmp3, AcceptedCmp4,AcceptedCmp5,
Marital_Status, Status, Total_Kids, Education, Kidhome, Teenhome.
"""

# Create a copy of the DataFrame to avoid modifying the original
data_dropped = data.copy()

# Drop the specified columns
columns_to_drop = ['Year_Birth', 'Dt_Customer', 'Complain', 'Response',
                  'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3',
                  'AcceptedCmp4', 'AcceptedCmp5', 'Marital_Status',
                  'Education', 'Kidhome', 'Teenhome']
data_dropped = data_dropped.drop(columns=columns_to_drop, axis=1)

# Check the shape of the data
print(data_dropped.shape)

# Display the first five rows of the data_model DataFrame
print(data_dropped.head())

# Calculate the correlation matrix
correlation_matrix = data_dropped.corr()

# Create the heatmap
plt.figure(figsize=(12, 10))  # Adjust figure size as needed
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Plot for data_dropped")
plt.show()

# Scaling the data

# Replace infinite values with NaN
data_dropped.replace([np.inf, -np.inf], np.nan, inplace=True)

# Impute with the median again because it provided an error.

data_dropped.fillna(data_dropped.median(), inplace=True)

# Scaling the data
# Initialize the Standard Scaler
scaler = StandardScaler()

# fit_transform the scaler function on new data
df_scaled = scaler.fit_transform(data_dropped)

# Converting the embeddings to a dataframe
df_scaled = pd.DataFrame(df_scaled, columns=data_dropped.columns)

# Display the first five rows of the scaled DataFrame
df_scaled.head()

# Distribution of Total Spending

plt.figure(figsize=(8, 6))
plt.hist(df_scaled['Total_Spent'], bins=20, edgecolor='black')
plt.title('Distribution of Total Spending')
plt.xlabel('Total Spent')
_ = plt.ylabel('Frequency')

# Distribution of Customer Tenure

plt.figure(figsize=(10, 6))
plt.hist(df_scaled['Customer_Tenure'], bins=20, edgecolor='black')
plt.xlabel('Customer Tenure')
plt.ylabel('Frequency')
_ = plt.title('Distribution of Customer Tenure')

"""## **Applying T-SNE and PCA to the data to visualize the data distributed in 2 dimensions**

### **Applying T-SNE**
"""

# Fitting T-SNE with number of components equal to 2 to visualize how data is distributed
tsne = TSNE(n_components=2, random_state=1, perplexity=35)        # Initializing T-SNE with number of component equal to 2, random_state=1, and perplexity=35

data_air_pol_tsne = tsne.fit_transform(df_scaled)                           # fit_transform T-SNE on new data

data_air_pol_tsne = pd.DataFrame(data_air_pol_tsne, columns=[0, 1])           # Converting the embeddings to a dataframe

plt.figure(figsize=(7, 7))                                                    # Scatter plot for two components

sns.scatterplot(x=0, y=1, data=data_air_pol_tsne)                             # Plotting T-SNE
plt.show()

"""**Observation and Insights:**
* The correlation heatmap revealed relationships between features. For example, higher education levels were positively correlated with income.
* DataFrame was reduced to 21 columns.
* T-SNE was applied to reduce the data to 2 dimensions for visualization. The scatter plot of the T-SNE embeddings hinted at the presence of potential clusters.

### **Applying PCA**

**Think about it:**
- Should we apply clustering algorithms on the current data or should we apply PCA on the data before applying clustering algorithms? How would this help?
* Aplying PCA is a good choice to reduce noises, improve computational efficiency and avoid the curse of dimensionality. In this case, PCA can be used to reduce the multicollinearity between the variables.
"""

# Defining the number of principal components to generate

n = 21

pca = PCA(n_components=n, random_state=1)

data_pca = pd.DataFrame(pca.fit_transform(df_scaled))  # fit_transform PCA on the scaled data

# The percentage of variance explained by each principal component is stored
exp_var = pca.explained_variance_ratio_

import copy

# Assign a deep copy of data_pca to data_dropped
data_dropped = copy.deepcopy(data_pca)

# Scatter plot for two components using the dataframe data_pca
plt.figure(figsize=(8, 8))
sns.scatterplot(x=data_pca[0], y=data_pca[1], data=data_pca)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("Scatter Plot of Data in First Two Principal Components")
plt.show()

"""**Observation and Insights:**
* Potential clusters;
* Some outliers.

## **K-Means**

**Think About It:**

- How do we determine the optimal K value from the elbow curve?
- Which metric can be used to determine the final K value?
"""

# Convert column names to strings
data_dropped.columns = data_dropped.columns.astype(str)

distortions = []                                                  # Create an empty list

K = range(2, 10)                                                  # Setting the K range from 2 to 10

for k in K:
    kmeanModel = KMeans(n_clusters=k,random_state=4)              # Initialize K-Means
    kmeanModel.fit(data_dropped)                                      # Fit K-Means on the data
    distortions.append(kmeanModel.inertia_)

# Plotting the elbow plot
plt.figure(figsize=(16, 8))                                            # Setting the plot size

plt.plot(K, distortions, "bx-")                                        # Plotting the K on X-axis and distortions on y-axis

plt.xlabel("k")                                                        # Title of x-axis

plt.ylabel("Distortion")                                               # Title of y-axis

plt.title("The Elbow Method showing the optimal k")                    # Title of the plot
plt.show()

# Finding silhouette score for each value of K
sil_score = []                                                             # Creating empty list
cluster_list = range(3, 7)                                                 # Creating a range from 3 to 7
for n_clusters in cluster_list:

    # Perform KMeans clustering on data_dropped
    kmeans = KMeans(n_clusters=n_clusters, random_state=1) # Changed n_clusters to reflect loop variable
    kmeans.fit(data_dropped)
    data_dropped["K_means_segments_3"] = kmeans.labels_

    # Calculate cluster profiles from data_dropped
    cluster_profile_KMeans_3 = data_dropped.groupby('K_means_segments_3').mean()

    # Fit and predict on the pca data
    preds = kmeans.fit_predict(data_dropped) # Removed unnecessary variable 'clusterer' & used the kmeans variable already defined

    # Calculate silhouette score - Hint: Use silhouette_score() function
    score = silhouette_score(data_dropped, preds)

    # Append silhouette score to empty list created above
    sil_score.append(score)

    # Print the silhouette score
    print( "For n_clusters = {}, the silhouette score is {}".format(n_clusters, score))

"""### **Applying KMeans on the PCA data and visualize the clusters**"""

kmeans = KMeans(n_clusters=3, random_state=1)                                # Initialize the K-Means algorithm with 3 clusters and random_state=1

kmeans.fit(data_dropped)                                     # Fitting on the data_pca

# adding K-Means clusters to data_pca, whole data and data_models
data_pca["K_means_segments_3"] = kmeans.labels_

data["K_means_segments_3"] = kmeans.labels_

data_dropped["K_means_segments_3"] = kmeans.labels_

# Checking the distribution
data_dropped["K_means_segments_3"].value_counts()

# Visualizing PCA data with clusters formed

def PCA_PLOT(x_index, y_index, PCA, cluster):
    sns.scatterplot(x=PCA.iloc[:, x_index], y=PCA.iloc[:, y_index], data=PCA, hue=cluster)

PCA_PLOT(0, 1, data_pca, "K_means_segments_3")
plt.show()

"""### **Cluster Profiling**"""

# Taking the cluster-wise mean of all the variables.
cluster_profile_KMeans_3 = data_dropped.groupby('K_means_segments_3').mean()
print(cluster_profile_KMeans_3.T.head()) # Transpose and show first few rows

# Highlighting the maximum average value among all the clusters for each of the variables
cluster_profile_KMeans_3.style.highlight_max(color="lightgreen", axis=0)

# Highlight minimum with light blue
cluster_profile_KMeans_3.style.highlight_min(color="lightblue", axis=0)

"""**Observations and Insights:**
* Three clusters.

### **Describe the characteristics of each cluster**
* Cluster 2 has the higher Income. Cluster 1 has the smaller.
* Cluster 1 has more kids. Cluster 0 has more teenagers.
* Cluster 1 has complained more and visited the website more frequently.
* Cluster 2 buys expenses are higher.

**Think About It:**
- Are the K-Means profiles providing any deep insights into customer purchasing behavior or which channels they are using?
- What is the next step to get more meaningful insights?
"""

# Columns to use in boxplot
col_for_box = ['Income','Kidhome','Teenhome','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Complain','Age','Family_Size','Expenses','NumTotalPurchases','Engaged_in_days','TotalAcceptedCmp','AmountPerPurchase']

# Creating boxplot for each of the variables
all_col = ['Income','Kidhome','Teenhome','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Complain','Age','Family_Size','Expenses', 'Total_Purchases', 'Customer_Tenure', 'Total_Offers_Accepted', 'Amount_per_Purchase'] #Removing the columns not in the data and using the created ones

plt.figure(figsize=(30, 50))

for i, variable in enumerate(all_col):
    plt.subplot(6, 4, i + 1)

    sns.boxplot(y=data[variable], x=data['K_means_segments_3'], showmeans=True)

    plt.tight_layout()

    plt.title(variable)

plt.show()

"""**Summary of each cluster:**

**Cluster 0: Medium Income. Medium Expenses. Higher Education
**Cluster 1: Lower Income. Lower Expenses. Lower Education.
**Cluster 2: higher income. higher expenses. Medium Education.
"""

# Dropping K_means_segments_3 from data_pca
data_pca.drop('K_means_segments_3', axis=1, inplace=True)

# Dropping K_means_segments_3 from data
data.drop('K_means_segments_3', axis=1, inplace=True)

"""KMeans with 5 clusters"""

# Initialize KMeans with 5 clusters and random_state=0
kmeans = KMeans(n_clusters=5, random_state=0)

# Fit the model to the data
kmeans.fit(data_pca)

# Adding k-means to data_pca, whole data and data_model
data_pca['K_means_segments'] = kmeans.labels_

data['K_means_segments'] = kmeans.labels_

data_model['K_means_segments'] = kmeans.labels_

# Checking the distribution
cluster_distribution = data_model['K_means_segments'].value_counts()
print(cluster_distribution)

# Plotting PCA
def PCA_PLOT(X, Y, PCA, cluster):
    sns.scatterplot(x=X, y=1, data=PCA, hue=cluster)

# Taking the cluster-wise mean of all the variables.
cluster_profile_KMeans_5 = data.groupby('K_means_segments').mean()

cluster_profile_KMeans_5.style.highlight_max(color="lightgreen", axis=0)

"""## **K-Medoids**"""

kmedo = KMedoids(n_clusters=5, random_state=1)           # Initializing K-Medoids with number of clusters as 5 and random_state=1

preds = kmedo.fit_predict(data_pca)            # Fit and predict K-Medoids using data_pca

score = silhouette_score(data_pca, preds)           # Calculate the silhouette score

print(score)

# Predicting on data_pca and adding K-Medoids cluster labels to the whole data
data['KMedoids_Clusters'] = kmedo.predict(data_pca)

# Predicting on data_pca and adding K-Medoids cluster labels to data_model
data_model['KMedoids_Clusters'] = kmedo.predict(data_pca)

# Predicting on data_pca and adding K-Medoids cluster labels to data_pca
data_pca['KMedoids_Clusters'] = kmedo.predict(data_pca)

# Check the distribution of K-Medoids clusters in data_model
cluster_distribution = data_model['KMedoids_Clusters'].value_counts()
print(cluster_distribution)

"""### **Visualize the clusters using PCA**"""

# Visualize K-Medoids clusters using PCA_PLOT
PCA_PLOT(0, 1, data_pca, 'KMedoids_Clusters')
plt.show()

"""### **Cluster Profiling**"""

# Calculate cluster-wise mean of all variables
cluster_profile_KMedoids = data.groupby('KMedoids_Clusters').mean()

# Highlight maximum average values
cluster_profile_KMedoids.style.highlight_max(color="lightgreen", axis=0)

# Highlight minimum average values
cluster_profile_KMedoids.style.highlight_min(color="lightblue", axis=0)

"""**Observations and Insights:**

* four clusters.

### **Characteristics of each cluster**

**Summary for each cluster:**
* Cluster 0: Higher Income. Higher Expenses. More Purchases.
* Cluster 1: Youngest group. More visits to website. Higher Complains. Lower education. Lower income.
* Cluster 2: More kids and bigger families. Lower total spent.
* Cluster 3: More teenagers at home. More web purchases and oldest group.
* Cluster 4: Higher Education. Higher complains. Higher amount per perchase.

**Observations and Insights:**

It looks like 5 clusters are more meaningful than 3.
"""

# Dropping KMedoids_Clusters from data_pca
data_pca.drop('KMedoids_Clusters', axis=1, inplace=True)

# Dropping KMedoids_Clusters from data
data.drop('KMedoids_Clusters', axis=1, inplace=True)

"""## **Hierarchical Clustering**

- Find the Cophenetic correlation for different distances with different linkage methods.
- Create the dendrograms for different linkages
- Explore different linkages with each distance metric
"""

# list of distance metrics
distance_metrics = ["euclidean", "chebyshev", "mahalanobis", "cityblock"]

# list of linkage methods
linkage_methods = ["single", "complete", "average"]

high_cophenet_corr = 0                                                 # Creating a variable by assigning 0 to it
high_dm_lm = [0, 0]                                                    # Creating a list by assigning 0's to it

for dm in distance_metrics:
    for lm in linkage_methods:
        Z = linkage(data_pca, metric=dm, method=lm)                    # Applying different linkages with different distance on data_pca
        c, coph_dists = cophenet(Z, pdist(data_pca))                   # Calculating cophenetic correlation
        print(
            "Cophenetic correlation for {} distance and {} linkage is {}.".format(
                dm.capitalize(), lm, c
            )
        )
        if high_cophenet_corr < c:                                     # Checking if cophenetic correlation is higher than previous score
            high_cophenet_corr = c                                     # Appending to high_cophenet_corr list if it is higher
            high_dm_lm[0] = dm                                         # Appending its corresponding distance
            high_dm_lm[1] = lm                                         # Appending its corresponding method or linkage

# Printing the combination of distance metric and linkage method with the highest cophenetic correlation
print(
    "Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.".format(
        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]
    )
)

# List of linkage methods
linkage_methods = ["single", "complete", "average"]

# Lists to save results of cophenetic correlation calculation
compare_cols = ["Linkage", "Cophenetic Coefficient"]

# To create a subplot image
fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))            # Setting the plot size

# We will enumerate through the list of linkage methods above
# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation
for i, method in enumerate(linkage_methods):
    Z = linkage(data_pca, metric="Cityblock", method=method)                  # Measures the distances between two clusters

    dendrogram(Z, ax=axs[i])
    axs[i].set_title(f"Dendrogram ({method.capitalize()} Linkage)")           # Title of dendrogram

    coph_corr, coph_dist = cophenet(Z, pdist(data_pca))                       # Finding cophenetic correlation for different linkages with city block distance
    axs[i].annotate(
        f"Cophenetic\nCorrelation\n{coph_corr:0.2f}",
        (0.80, 0.80),
        xycoords="axes fraction",
    )

"""**Think about it:**

- Can we clearly decide the number of clusters based on where to cut the dendrogram horizontally? It depends on the dataset and the desired level of glanularity.
- What is the next step in obtaining number of clusters based on the dendrogram?
- Are there any distinct clusters in any of the dendrograms? Yes, there are distinct clusters at the Complete Linkage.
"""

# Calculate the distance matrix using Chebyshev distance
distance_matrix = pdist(df_scaled, metric='chebyshev')

# Create dendrograms for different linkages
plt.figure(figsize=(20, 10))

# Single linkage
plt.subplot(1, 3, 1)
dendrogram = sch.dendrogram(sch.linkage(distance_matrix, method='single'))
plt.title('Dendrogram (Single Linkage, Chebyshev)')

# Complete linkage
plt.subplot(1, 3, 2)
dendrogram = sch.dendrogram(sch.linkage(distance_matrix, method='complete'))
plt.title('Dendrogram (Complete Linkage, Chebyshev)')

# Average linkage
plt.subplot(1, 3, 3)
dendrogram = sch.dendrogram(sch.linkage(distance_matrix, method='average'))
plt.title('Dendrogram (Average Linkage, Chebyshev)')

plt.tight_layout()
plt.show()

# Convert column names to strings
data_pca.columns = data_pca.columns.astype(str)

# Initialize Agglomerative Clustering with affinity (distance) as Euclidean, linkage as 'ward' with clusters=5
HCmodel = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')

# Fit on data_pca
HCmodel.fit(data_pca)

# Scale the data
# Initialize the Standard Scaler
scaler = StandardScaler()
# Select only numerical columns for scaling
numerical_data = data.select_dtypes(include=['number'])

# Replace infinite values with NaN
numerical_data.replace([np.inf, -np.inf], np.nan, inplace=True)

# Impute NaN values with the median
numerical_data.fillna(numerical_data.median(), inplace=True)

df_scaled = scaler.fit_transform(numerical_data)

# If you need a DataFrame with scaled numerical features and original other columns
df_scaled = pd.DataFrame(df_scaled, columns=numerical_data.columns, index=data.index)
df_scaled = df_scaled.join(data.select_dtypes(exclude=['number']))

# Compute condensed distance matrix
distance_matrix = pdist(df_scaled.select_dtypes(include=['number']), metric='euclidean')

# Compute linkage matrix
linkage_matrix = sch.linkage(distance_matrix, method='ward')

# Plot dendrogram
plt.figure(figsize=(10, 7))
dendrogram = sch.dendrogram(linkage_matrix)
plt.title("Dendrogram (Ward Linkage, Euclidean)")
plt.show()

# Select only numerical features for distance calculation
numerical_features = df_scaled.select_dtypes(include=['number'])
distance_matrix = pdist(numerical_features, metric='euclidean')

# Create dendrograms for different linkages
plt.figure(figsize=(20, 10))

# Single linkage
plt.subplot(1, 3, 1)
dendrogram = sch.dendrogram(sch.linkage(distance_matrix, method='single'))
plt.title('Dendrogram (Single Linkage, euclidean)')

# Complete linkage
plt.subplot(1, 3, 2)
dendrogram = sch.dendrogram(sch.linkage(distance_matrix, method='complete'))
plt.title('Dendrogram (Complete Linkage, euclidean)')

# Average linkage
plt.subplot(1, 3, 3)
dendrogram = sch.dendrogram(sch.linkage(distance_matrix, method='average'))
plt.title('Dendrogram (Average Linkage, euclidean)')

plt.tight_layout()
plt.show()

# Add cluster labels to data_pca
data_pca['Agglomerative_Clusters'] = HCmodel.labels_

# Add cluster labels to the whole data (data)
data['Agglomerative_Clusters'] = HCmodel.labels_

# Add cluster labels to data_model
data_model['Agglomerative_Clusters'] = HCmodel.labels_

# Check the distribution of Agglomerative Clustering clusters in data_model
cluster_distribution = data_model['Agglomerative_Clusters'].value_counts()
print(cluster_distribution)

"""### **Visualize the clusters using PCA**"""

# Visualize Agglomerative Clusters using PCA_PLOT
agl_plot = PCA_PLOT(0, 1, data_pca, 'Agglomerative_Clusters')
plt.show()

"""### **Cluster Profiling**"""

# Calculate cluster-wise mean of all variables
cluster_profile_Agglomerative = data.groupby('Agglomerative_Clusters').mean()

# Highlight maximum average values
cluster_profile_Agglomerative.style.highlight_max(color="lightgreen", axis=0)

# Highlight minimum average values
cluster_profile_Agglomerative.style.highlight_min(color="lightblue", axis=0)

# Get a list of all numerical columns (variables)
all_col = data.select_dtypes(include=['number']).columns

# Calculate the number of rows and columns for the subplot grid
num_cols = 4  # Number of columns in the grid
num_rows = int(np.ceil(len(all_col) / num_cols))  # Calculate rows to fit all columns

# Create a figure with subplots, dynamically adjusting the grid size
plt.figure(figsize=(30, 5 * num_rows))  # Adjust figure height based on rows

# Loop through each variable and create a box plot
for i, variable in enumerate(all_col):
    plt.subplot(num_rows, num_cols, i + 1)  # Use calculated rows and columns
    sns.boxplot(y=data[variable], x=data['Agglomerative_Clusters'], showmeans=True)
    plt.tight_layout()
    plt.title(variable)

plt.show()

"""**Observations and Insights:**

### **Characteristics of each cluster**

*  Cluster 0: High income. Higher total spent. Higher offers accepted. higher total purchases. Lower kids and teens. Higher meat and wine. Lower complains.
* Cluster 1: Higher MntFruits. Higher fish products. higher gold products. Higher sweet products. Higher store purchases. Lower web visits. Cluster: 2: Higher education. Higher teens. Oldest group. Lower amount per purchase.
* Cluster 3: Youngest segment. Higher kids. Higher web visits. Higher complains. Lower Mntwines and lower Mntmeat products.
* Cluster 4: bigger families. lower fish, sweet and gold produts.

**Summary of each cluster:**
Cluster 0: High income. Higher total spent. Higher offers accepted. higher total purchases. Lower kids and teens. Higher meat and wine. Lower complains.
Cluster 1: Higher MntFruits. Higher fish products. higher gold products. Higher sweet products. Higher store purchases. Lower web visits. Cluster: 2: Higher education. Higher teens. Oldest group. Lower amount per purchase.
Cluster 3: Youngest segment. Higher kids. Higher web visits. Higher complains. Lower Mntwines and lower Mntmeat products.
Cluster 4: bigger families. lower fish, sweet and gold produts.

## **DBSCAN**

DBSCAN is a very powerful algorithm for finding high-density clusters, but the problem is determining the best set of hyperparameters to use with it. It includes two hyperparameters, `eps`, and `min samples`.

Since it is an unsupervised algorithm, you have no control over it, unlike a supervised learning algorithm, which allows you to test your algorithm on a validation set. The approach we can follow is basically trying out a bunch of different combinations of values and finding the silhouette score for each of them.
"""

# Initializing lists
eps_value = [2,3]                       # Taking random eps value
min_sample_values = [6,20]              # Taking random min_sample value

# Creating a dictionary for each of the values in eps_value with min_sample_values
res = {eps_value[i]: min_sample_values for i in range(len(eps_value))}

# Finding the silhouette_score for each of the combinations
high_silhouette_avg = 0                                               # Assigning 0 to the high_silhouette_avg variable
high_i_j = [0, 0]                                                     # Assigning 0's to the high_i_j list
key = res.keys()                                                      # Assigning dictionary keys to a variable called key
for i in key:
    z = res[i]                                                        # Assigning dictionary values of each i to z
    for j in z:
        db = DBSCAN(eps=i, min_samples=j).fit(data_pca)               # Applying DBSCAN to each of the combination in dictionary
        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
        core_samples_mask[db.core_sample_indices_] = True
        labels = db.labels_
        silhouette_avg = silhouette_score(data_pca, labels)           # Finding silhouette score
        print(
            "For eps value =" + str(i),
            "For min sample =" + str(j),
            "The average silhoutte_score is :",
            silhouette_avg,                                          # Printing the silhouette score for each of the combinations
        )
        if high_silhouette_avg < silhouette_avg:                     # If the silhouette score is greater than 0 or the previous score, it will get appended to the high_silhouette_avg list with its combination of i and j
            high_i_j[0] = i
            high_i_j[1] = j

# Printing the highest silhouette score
print("Highest_silhoutte_avg is {} for eps = {} and min sample = {}".format(high_silhouette_avg, high_i_j[0], high_i_j[1]))

eps = 0.5  # Epsilon
min_samples = 5  # Minimum number of samples in a neighborhood to be considered a core point

# Apply DBSCAN
dbs = DBSCAN(eps=eps, min_samples=min_samples)
dbs.fit(data_pca)  # Assuming data_pca is your preprocessed data

# Get cluster labels
labels = dbs.labels_

# Add cluster labels to your DataFrame
data_pca['DBSCAN_Clusters'] = labels
data['DBSCAN_Clusters'] = labels

# Explore the results
print(data['DBSCAN_Clusters'].value_counts())
# ... (further analysis and visualization)

# Apply DBSCAN using the above hyperparameter values
dbs = DBSCAN(eps=1.5, min_samples=15)  # Assuming eps=1.5 and min_samples=15 from previous steps

# fit_predict on data_pca and add DBSCAN cluster labels to the whole data
data['DBSCAN_Clusters'] = dbs.fit_predict(data_pca)

# fit_predict on data_pca and add DBSCAN cluster labels to data_model
data_model['DBSCAN_Clusters'] = dbs.fit_predict(data_pca)

# fit_predict on data_pca and add DBSCAN cluster labels to data_pca
data_pca['DBSCAN_Clusters'] = dbs.fit_predict(data_pca)

# Check the distribution of DBSCAN clusters in data_model
cluster_distribution = data_model['DBSCAN_Clusters'].value_counts()
print(cluster_distribution)

"""### **Apply DBSCAN for the best hyperparameter and visualize the clusters from PCA**"""

def PCA_PLOT(X, Y, PCA, cluster):
    """
    Visualizes clusters using PCA.

    Parameters:
    - X (int): Index of the first principal component for visualization.
    - Y (int): Index of the second principal component for visualization.
    - PCA (pd.DataFrame): DataFrame containing PCA components.
    """
    plt.figure(figsize=(8, 8))  # Adjust figure size as needed
    sns.scatterplot(x=PCA.iloc[:, X], y=PCA.iloc[:, Y], hue=PCA[cluster], palette='viridis')
    plt.xlabel(f"Principal Component {X + 1}")  # Add 1 to index for display
    plt.ylabel(f"Principal Component {Y + 1}")

"""**Observations and Insights:**

**Think about it:**

- Changing the eps and min sample values will result in different DBSCAN results? Can we try more value for eps and min_sample?

Yes! Changing the parameters affects the outcome.

### **Characteristics of each cluster**
"""

# Dropping labels we got from DBSCAN since we will be using PCA data for prediction
# Hint: Use axis=1 and inplace=True
data_pca.drop('DBSCAN_Clusters', axis=1, inplace=True)
data.drop('DBSCAN_Clusters', axis=1, inplace=True)

"""## **Gaussian Mixture Model**"""

gmm = GaussianMixture(n_components=5, random_state=1) # Initialize Gaussian Mixture Model with number of clusters as 5 and random_state=1

preds = gmm.fit_predict(data_pca)            # Fit and predict Gaussian Mixture Model using data_pca

score = silhouette_score(data_pca, preds)           # Calculate the silhouette score

print(score)                   # Print the score

# Predicting on data_pca and add Gaussian Mixture Model cluster labels to the whole data
data['GMM_Clusters'] = gmm.predict(data_pca)

# Predicting on data_pca and add Gaussian Mixture Model cluster labels to data_model
data_model['GMM_Clusters'] = gmm.predict(data_pca)

# Predicting on data_pca and add Gaussian Mixture Model cluster labels to data_pca
data_pca['GMM_Clusters'] = gmm.predict(data_pca)

# Check the distribution of GMM clusters in data_model
cluster_distribution = data_model['GMM_Clusters'].value_counts()
print(cluster_distribution)

"""### **Visualize the clusters using PCA**"""

def PCA_PLOT(X, Y, PCA, cluster):
    """
    Visualizes clusters using PCA.

    Args:
        X: The column index for the x-axis (usually the first principal component).
        Y: The column index for the y-axis (usually the second principal component).
        PCA: The PCA transformed data.
        cluster: The cluster assignments.
    """
    sns.scatterplot(x=X, y=Y, data=PCA, hue=cluster)

    print(f"Silhouette Score: {silhouette_score(PCA, cluster)}")

"""### **Cluster Profiling**"""

# Calculate cluster-wise mean of all variables for Agglomerative Clustering
cluster_profile_Agglomerative = data.groupby('Agglomerative_Clusters').mean()

# Display the results
print(cluster_profile_Agglomerative)

# Highlight the maximum average value among all the clusters for each of the variables
cluster_profile_Agglomerative.style.highlight_max(color="lightgreen", axis=0)

# Highlight the minimum average value among all the clusters for each of the variables
cluster_profile_Agglomerative.style.highlight_min(color="lightblue", axis=0)

# Get all numerical columns for boxplots
all_col = data.select_dtypes(include=['number']).columns

# Create boxplots
num_cols = len(all_col)
num_rows = (num_cols + 3) // 4

plt.figure(figsize=(30, 5 * num_rows))

for i, variable in enumerate(all_col):
    plt.subplot(num_rows, 4, i + 1)
    sns.boxplot(y=data[variable], x=data['Agglomerative_Clusters'], showmeans=True)
    plt.tight_layout()
    plt.title(variable)

plt.show()

"""### **Characteristics of each cluster**

**Summary of each cluster:**
*Cluster 0: Higher Income, Higher Mntwines. Higher Mntmeatproducts. Higher propensity to accept campaigns. Higher Total spent.
*Cluster 1: Higher marital status. Higher mntfruits. Higher mntfish. Higher Mntsweetproducts. Higher Nhtgoldproducts.
*Cluster 2: Higher education. Higher teens. Higher deals purchases and higher webpurchases. Oldest group.
*Cluster 3: Youngest group. higher kids. Higher web visits. Higher complains.  
*Cluster 4: Lower Marital status. Higher kids. Higher family size. Lower mnt fruits. lower fish. lower sweet and lower golds.

## **Conclusion and Recommendations**

**1. Comparison of various techniques and their relative performance based on chosen Metric (Measure of success)**:
- How do different techniques perform? Which one is performing relatively better?
- K-Medoids showed better performance.
- Is there scope to improve the performance further?
- There are scope for improving the performance.

**2. Refined insights**:
- What are the most meaningful insights from the data relevant to the problem?
* Customer segmentation was possible.
* High-income consumers spend more in products like wine, meat and gold.
* Customers exhibit varying preferences for purchasing channels.
* Clusters exhibiting higher complaint rates highlight potential areas for improvement in customer satisfaction.

**3. Proposal for the final solution design:**
- What model do you propose to be adopted? Why is this the best solution to adopt?
- My propose is to adopt k-medoids because it performed better than k-Means and hierarchical clustering. It provided meaninful insights and it is easy to interpretate.
"""