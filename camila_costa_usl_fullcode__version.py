# -*- coding: utf-8 -*-
"""Camila-Costa_USL_Fullcode__Version.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yMd84ICFIfmvvl4xabU9dR1XrmppC-SA

# **Unsupervised Learning Project: AllLife Bank Customer Segmentation**

# **Marks: 30**

Welcome to the project on Unsupervised Learning. We will be using **Credit Card Customer Data** for this project.

--------------------------------
## **Context**
-------------------------------

**AllLife Bank wants to focus on its credit card customer base** in the next financial year. They have been advised by their marketing research team, that the penetration in the market can be improved. Based on this input, the marketing team proposes to run personalized campaigns to target new customers as well as upsell to existing customers.

Another insight from the market research was that the customers perceive the support services of the bank poorly. Based on this, the operations team wants to upgrade the service delivery model, to ensure that customers' queries are resolved faster. The head of marketing and the head of delivery, both decide to reach out to the Data Science team for help.


----------------------------
## **Objective**
-----------------------------

**Identify different segments in the existing customer base**, taking into account their spending patterns as well as past interactions with the bank.

--------------------------
## **About the data**
--------------------------

Data is available on customers of the bank with their credit limit, the total number of credit cards the customer has, and different channels through which the customer has contacted the bank for any queries. These different channels include visiting the bank, online, and through a call center.

- **Sl_no** - Customer Serial Number
- **Customer Key** - Customer identification
- **Avg_Credit_Limit**	- Average credit limit (currency is not specified, you can make an assumption around this)
- **Total_Credit_Cards** - Total number of credit cards
- **Total_visits_bank**	- Total bank visits
- **Total_visits_online** - Total online visits
- **Total_calls_made** - Total calls made

## **Importing libraries and overview of the dataset**

**Note:** Please make sure you have installed the sklearn_extra library before running the below cell. If you have not installed the library, please run the below code to install the library:

!pip install scikit-learn-extra
"""

# Installing library
!pip install scikit-learn-extra

# Importing all the necessary packages

import pandas as pd

import numpy as np

import matplotlib.pylab as plt

import seaborn as sns

# To scale the data using z-score
from sklearn.preprocessing import StandardScaler

# Importing clustering algorithms
from sklearn.cluster import KMeans

from sklearn.mixture import GaussianMixture

from sklearn_extra.cluster import KMedoids

import warnings
warnings.filterwarnings("ignore")

"""### **Loading the data**"""

# Import drive
from google.colab import drive
drive.mount("/content/drive")

# Importing file
data = pd.read_excel("/content/drive/My Drive/Python/MIT/Elective Project/2/Credit+Card+Customer+Data.xlsx")
data.head()

"""### **Data Overview**

- Observations
- Sanity checks



"""

data.info()

data.nunique()

"""##**Cleaning Data**"""

# There are some duplicates in the column 'Customer Key'.

duplicate_keys = data['Customer Key'].duplicated(keep=False)

data[duplicate_keys]

# drop duplicate keys

data = data.drop_duplicates(subset=['Customer Key'], keep='first')

# drop duplicate keys
data.drop(columns = ['Sl_No', 'Customer Key'], inplace = True)

data = data[~data.duplicated()]

data.shape

"""## **Data Preprocessing and Exploratory Data Analysis**

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.
- Check and drop the duplicate customer keys
- Drop the variables that are not required for the analysis
- Check duplicate rows and remove them.


**Questions:**

1. How does the distribution and outliers look for each variable in the data?
2. How are the variables correlated with each other?

#### **Check the summary Statistics**
"""

data.describe().T

# Checking distribution and outliers
for col in data.columns:
    print(col)

    print('Skew :', round(data[col].skew(), 2))

    plt.figure(figsize = (15, 4))

    plt.subplot(1, 2, 1)

    data[col].hist(bins=20, grid=False) # Complete the code with hist()

    plt.ylabel('count')

    plt.subplot(1, 2, 2)

    sns.boxplot(x = data[col]) # Complete the code

    plt.show()

"""**Observations:___________**
* Outliers on primarily on the higher end. Can be a possibility for other analysis.
* Data is not normally distributed.
"""

# Checking correlation
plt.figure(figsize = (8, 8))

sns.heatmap(data.corr(), annot = True, fmt = '0.2f')

plt.show()

"""**Observations:**

* Positive correlation between Avg_Credit_Limit and Total_Credit_Cards and Avg_Credit_Limit and  Total_visits_online.
* Avg_Credit_Limit is negatively correlated with Total_calls_made and Total_visits_bank.
* Total_visits_bank, Total_visits_online, Total_calls_made are negatively correlated. It implies that majority of customers use only one of these channels to contact the bank.

#### **Scaling the data**
"""

scaler = StandardScaler()

# Standardize the data to have a mean of ~0 and a variance of 1
data_scaled = StandardScaler().fit_transform(data)

"""#### **Applying PCA on scaled data**"""

from sklearn.decomposition import PCA

n = data.shape[1]

# Create a PCA instance: pca
pca = PCA(n_components=n)

principal_components = pca.fit_transform(data_scaled)

# Save components to a DataFrame
data_pca = pd.DataFrame(principal_components, columns = data.columns)

# Creating copy of the data to store labels from each algorithm

data_copy = data_pca.copy(deep = True)

"""## **K-Means**

Let us now fit the K-means algorithm on our pca components and find out the optimum number of clusters to use.

We will do this in 3 steps:
1. Initialize a dictionary to store the Sum of Squared Error (SSE) for each K
2. Run for a range of Ks and store SSE for each run
3. Plot the SSE vs K and plot the elbow curve
"""

# 1
sse = {}

# 2 - iterate for a range of Ks and fit the pca components to the algorithm.
for k in range(1, 10):
    kmeans = KMeans(n_clusters = k, max_iter = 1000, random_state = 1).fit(data_pca)
    sse[k] = kmeans.inertia_     # Use inertia attribute from the clustering object and store the inertia value for that K

# 3
plt.figure()

plt.plot(list(sse.keys()), list(sse.values()), 'bx-')

plt.xlabel("Number of cluster")

plt.ylabel("SSE")

plt.show()

"""- Interpret the above elbow plot and state the reason for choosing the particular value of K
- Fit the K-means algorithms on the pca components with the number of clusters for the chosen value of K

Interpretation
The elbow point suggests a good balance between clusters.
K=3 or k=4
"""

# Applying the K-Means algorithm
kmeans = KMeans(n_clusters=3, random_state=1)   #Assuming K=3 as per previous discussion

# Applying PCA transformation and store it in 'pca_data'
pca_data = pca.transform(data)  # Assuming 'pca' is your fitted PCA object and 'data' is your original data

# Fitting the kmeans function on the pca components
kmeans.fit(pca_data)    # Now 'pca_data' is defined and contains the PCA components

# Adding predicted labels to the original data and the copied data

# Saving the predictions on the pca components from K-Means
data_copy['Labels'] = kmeans.labels_
data['Labels'] = kmeans.labels_

"""#### **Create the cluster profiles using the summary statistics and box plots for each label**"""

# Number of observations in each cluster
data.Labels.value_counts()

# Calculating summary statistics of the original data for each label
mean = data.groupby('Labels').mean()

median = data.groupby('Labels').median()

df_kmeans = pd.concat([mean, median], axis = 0)

df_kmeans.index = ['group_0 Mean', 'group_1 Mean', 'group_2 Mean', 'group_0 Median', 'group_1 Median', 'group_2 Median']

df_kmeans.T

sns.scatterplot(x='Avg_Credit_Limit', y='Total_Credit_Cards', hue='Labels', data=data, palette='viridis')
plt.title('K-Means Clusters')
plt.show()

"""**Cluster Profiles:**

Cluster 0: low engagement and low credit. Lowest median. Possibly newer customers with lower financial activity.
Cluster 1: High Engagement and moderate credit. They use the bank services and they prefer bank visits and calls. They have moderate total_credit_cards and average credit limite.
Cluster 2: High credit and online users. This group has the highest median on avg_credit_limit and total_credit_cards. They interact more on online channels.

## **Gaussian Mixture Model**

Let's now create clusters using the Gaussian Mixture Model.

- Apply the Gaussian Mixture Model algorithm on the pca components
"""

# Applying the Gaussian Mixture algorithm on the pca components with n_components=3 and random_state=1
gmm = GaussianMixture(n_components=3, random_state=1)  # Initialize GMM with desired parameters


# Fitting the model on the pca components
gmm.fit(data_pca)  # Fit the GMM model to your PCA data (data_pca)

data_copy['GmmLabels'] = gmm.predict(data_pca)  # Assign cluster labels to data_copy

data['GmmLabels'] = gmm.predict(data_pca)  # Assign cluster labels to original data

# Number of observations in each cluster
data.GmmLabels.value_counts()

"""#### **Create the cluster profiles using the summary statistics and box plots for each label**

"""

# Calculating the summary statistics of the original data for each label
original_features = ["Avg_Credit_Limit", "Total_Credit_Cards", "Total_visits_bank", "Total_visits_online", "Total_calls_made"]

mean = data.groupby('GmmLabels').mean()

median = data.groupby('GmmLabels').median()

df_gmm = pd.concat([mean, median], axis = 0)

df_gmm.index = ['group_0 Mean', 'group_1 Mean', 'group_2 Mean', 'group_0 Median', 'group_1 Median', 'group_2 Median']

df_gmm[original_features].T

"""#### **Compare the clusters from both algorithms - K-means and Gaussian Mixture Model**"""

# Plotting boxplots with the new GMM based labels

features_with_lables = ["Avg_Credit_Limit", "Total_Credit_Cards", "Total_visits_bank", "Total_visits_online", "Total_calls_made", "GmmLabels"]

data_copy[features_with_lables].boxplot(by = 'GmmLabels', layout = (1, 5),figsize = (20, 7))

plt.show()

"""**Comparing Clusters:____________**

K-means
Cluster 0: low engagement and low credit. Lowest median. Possibly newer customers with lower financial activity.
Cluster 1: High Engagement and moderate credit. They use the bank services and they prefer bank visits and calls. They have moderate total_credit_cards and average credit limite.
Cluster 2: High credit and online users. This group has the highest median on avg_credit_limit and total_credit_cards. They interact more on online channels.

Gaussian Mixture Model
Cluster 0: with higher financial capacity, reflected in their larger credit limits and credit card ownership. They are also relatively active in terms of bank visits and online interactions.
Cluster 1: moderate financial capacity, exhibiting mid-range metrics across all dimensions.
Cluster 2: low credit limits, fewer credit cards, and minimal interaction in terms of calls, bank visits, or online activity.

## **K-Medoids**

- Apply the K-Medoids clustering algorithm on the pca components
"""

# Apply the K-Medoids algorithm on the pca components with n_components=3 and random_state=1
kmedo = KMedoids(n_clusters=3, random_state=1)  # Complete the code

# Fit the model on the pca components
kmedo.fit(data_pca)

data_copy['kmedoLabels'] = kmedo.predict(data_pca)

data['kmedoLabels'] = kmedo.predict(data_pca)

# Number of observations in each cluster
data.kmedoLabels.value_counts()

"""#### **Create cluster profiles using the summary statistics and box plots for each label**"""

# Calculating summary statistics of the original data for each label
mean = data.groupby('kmedoLabels').mean()

median = data.groupby('kmedoLabels').median()

df_kmedoids = pd.concat([mean, median], axis = 0)

df_kmedoids.index = ['group_0 Mean', 'group_1 Mean', 'group_2 Mean', 'group_0 Median', 'group_1 Median', 'group_2 Median']

df_kmedoids[original_features].T

# Plotting boxplots with the new K-Medoids based labels

features_with_lables = ["Avg_Credit_Limit", "Total_Credit_Cards", "Total_visits_bank", "Total_visits_online", "Total_calls_made", "kmedoLabels"]

data_copy[features_with_lables].boxplot(by = 'kmedoLabels', layout = (1, 5), figsize = (20, 7))

plt.show()

"""**Cluster Profiles:**
Cluster 0: Low financial capacity, minimal engagement in financial activities.
Cluster 1: Moderate financial behavior, balanced engagement.
Cluster 2: High financial capacity, frequent users of financial services and tools.

#### **Compare the clusters from K-Means and K-Medoids**
"""

comparison = pd.concat([df_kmedoids, df_kmeans], axis = 1)[original_features]

comparison

"""**Comparing Clusters:**

Both algorithms identify the same financial hierarchy (Cluster 0 < Cluster 1 < Cluster 2).

Both algorithms identified similar customer segments based on financial capacity and engagement levels. However, K-Medoids seems to have produced slightly more distinct and well-separated clusters, particularly for the low and high financial capacity groups.

## **Conclusions and Business Recommendations**
* Clusters can help to segment clients and offer better services.
* Improve customer experience in all channels, especially digital channels. It can strengthen the relation between AllLife Bank and customers.
* Offer credit products based on client segmentation.
"""