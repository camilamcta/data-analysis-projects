# -*- coding: utf-8 -*-
"""travel-experience.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15a0appDNHSYUeFjGRUnMBcdWkcLl-FdJ

<h1>Shinkansen Travel Experience</h>
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

from google.colab import drive
drive.mount('/content/drive')

travel_df = pd.read_csv('/content/drive/My Drive/hackaton/Traveldata_train_(2).csv')
travel_df_test = pd.read_csv('/content/drive/My Drive/hackaton/Traveldata_test_(2).csv')
survey_df = pd.read_csv('/content/drive/My Drive/hackaton/Surveydata_train_(2).csv')
survey_df_test = pd.read_csv('/content/drive/My Drive/hackaton/Surveydata_test_(2).csv')

"""The goal of the problem is to predict whether a passenger was satisfied or not considering his/her overall experience of traveling on the Shinkansen Bullet Train.

Dataset:

The problem consists of 2 separate datasets: Travel data & Survey data. Travel data has information related to passengers and attributes related to the Shinkansen train, in which they traveled. The survey data is aggregated data of surveys indicating the post-service experience. expecting to treat both these datasets as raw data and perform any necessary data cleaning/validation steps as required.

## Analyzing small portion of raw data
"""

display(survey_df.head())
display(survey_df_test.head())

display(survey_df.info())
display(survey_df_test.info())

display(survey_df.iloc[:,2:17].describe(include = 'all'))
display(survey_df_test.iloc[:,1:16].describe(include = 'all'))

display(survey_df.isna().sum())
display(survey_df_test.isna().sum())

display(travel_df.head())
display(travel_df_test.head())

display(travel_df.info())
display(travel_df_test.info())

display(travel_df.iloc[:,1:9].describe(include ='all'))
display(travel_df_test.iloc[:,1:9].describe(include ='all'))

"""## Display the total number of missing values in each column of the travel_df and travel_df_test dataframes."""

display(travel_df.isna().sum())
display(travel_df_test.isna().sum())

sns.pairplot(travel_df.iloc[:,1:9], corner= True, hue = 'Customer_Type')
plt.show()

merged_df = pd.merge(survey_df, travel_df, on= 'ID')
merged_df_test = pd.merge(survey_df_test, travel_df_test, on= 'ID')

merged_df = merged_df.assign(Seat_Comfort=merged_df['Seat_Comfort'].fillna('Acceptable'))
display(merged_df['Seat_Comfort'].isna().sum()) #imputing 'acceptable' value for 'NaN'

merged_df_test = merged_df_test.assign(Seat_Comfort=merged_df_test['Seat_Comfort'].fillna('Acceptable'))
display(merged_df_test['Seat_Comfort'].isna().sum()) #imputing 'acceptable' value for 'NaN'

# convert Arrival_Time_Convenient to numeric values for mean calculation
arrival_time_map = {'Poor': 0, 'Needs Improvement': 1, 'Acceptable': 2, 'Good': 3, 'Excellent': 4}

# For training data
display(merged_df.groupby(merged_df['Arrival_Time_Convenient'].map(arrival_time_map))['Arrival_Delay_in_Mins'].mean())
display(merged_df['Arrival_Time_Convenient'].value_counts())

# For test data
display(merged_df_test.groupby(merged_df_test['Arrival_Time_Convenient'].map(arrival_time_map))['Arrival_Delay_in_Mins'].mean())
display(merged_df_test['Arrival_Time_Convenient'].value_counts())

merged_df = merged_df.assign(Arrival_Time_Convenient=merged_df['Arrival_Time_Convenient'].fillna('Good'))
display(merged_df['Arrival_Time_Convenient'].isna().sum()) #imputing 'good' value for 'NaN'

merged_df_test = merged_df_test.assign(Arrival_Time_Convenient=merged_df_test['Arrival_Time_Convenient'].fillna('Good'))
display(merged_df_test['Arrival_Time_Convenient'].isna().sum())

#imputing all columns from survey and Gender and Frequency from travel in merged_df with highest frequency value
for i in range(5,19):
    most_frequent = merged_df.iloc[:,i].value_counts().index[0]
    merged_df.iloc[:,i] = merged_df.iloc[:,i].fillna(most_frequent)

for i in range(4,18):
    most_frequent = merged_df_test.iloc[:,i].value_counts().index[0]
    merged_df_test.iloc[:,i] = merged_df_test.iloc[:,i].fillna(most_frequent)

"""<h4>the following code discretizes the 'Age' feature into five age groups and assigns a label to each group, which will help in creating more meaningful categories for analysis and modeling.</h4>"""

merged_df['Age'] = pd.cut(merged_df['Age'], 5, labels = ['25', '35', '45', '60', '80'])
merged_df_test['Age'] = pd.cut(merged_df_test['Age'], 5, labels = ['25', '35', '45', '60', '80'])

display("Age:", merged_df.Age.unique())
display("Age:", merged_df_test.Age.unique())

np.where(merged_df['Customer_Type'] == 'Loyal Customer', merged_df['Age'], merged_df['Age'].fillna('60', inplace = True))
np.where(merged_df['Customer_Type'] == 'Disloyal Customer', merged_df['Age'], merged_df['Age'].fillna('25', inplace = True))
np.where(merged_df_test['Customer_Type'] == 'Loyal Customer', merged_df_test['Age'], merged_df_test['Age'].fillna('60', inplace = True))
np.where(merged_df_test['Customer_Type'] == 'Disloyal Customer', merged_df_test['Age'], merged_df_test['Age'].fillna('25', inplace = True))

merged_df.groupby(['Travel_Class', 'Type_Travel']).count()['ID']

#Check for Nan values
merged_df[merged_df['Type_Travel'].isna()]['Travel_Class']

merged_df['Type_Travel'] = np.where(merged_df['Travel_Class'] == 'Business', merged_df['Type_Travel'], 'Business travel')
merged_df['Type_Travel'] = np.where(merged_df['Travel_Class'] == 'Eco', merged_df['Type_Travel'], 'Personal Travel')

merged_df_test['Type_Travel'] = np.where(merged_df_test['Travel_Class'] == 'Business', merged_df_test['Type_Travel'], 'Business travel')
merged_df_test['Type_Travel'] = np.where(merged_df_test['Travel_Class'] == 'Eco', merged_df_test['Type_Travel'], 'Personal Travel')

merged_df = merged_df.assign(
    Departure_Delay_in_Mins=merged_df['Departure_Delay_in_Mins'].fillna(merged_df['Departure_Delay_in_Mins'].mean()),
    Arrival_Delay_in_Mins=merged_df['Arrival_Delay_in_Mins'].fillna(merged_df['Arrival_Delay_in_Mins'].mean())
)

merged_df_test = merged_df_test.assign(
    Departure_Delay_in_Mins=merged_df_test['Departure_Delay_in_Mins'].fillna(merged_df_test['Departure_Delay_in_Mins'].mean()),
    Arrival_Delay_in_Mins=merged_df_test['Arrival_Delay_in_Mins'].fillna(merged_df_test['Arrival_Delay_in_Mins'].mean())
)

display(merged_df.isna().sum())
display(merged_df_test.isna().sum())

"""<h4>The following code replaces the string values of ordinal categories with corresponding numerical values, in order to perform computations and analysis on these categorical variables.</h4>"""

#Encoded column value replacement for ordinal categories in merged_df
cols_to_replace = ['Seat_Comfort', 'Arrival_Time_Convenient', 'Catering', 'Onboard_Wifi_Service',
                   'Onboard_Entertainment', 'Online_Support', 'Ease_of_Online_Booking',
                   'Onboard_Service','Legroom', 'CheckIn_Service',  'Cleanliness',
                   'Online_Boarding', 'Baggage_Handling']

# Create mapping dictionary
value_map = {'Excellent': 5, 'Good': 4, 'Acceptable': 3,
            'Needs Improvement': 2, 'Poor': 1, 'Extremely Poor': 0}

# Update merged_df using assign
for col in cols_to_replace:
    merged_df = merged_df.assign(**{
        col: merged_df[col].replace(value_map)
    })

# Update merged_df_test using assign
for col in cols_to_replace:
    merged_df_test = merged_df_test.assign(**{
        col: merged_df_test[col].replace(value_map)
    })

merged_df_test.info()

"""<h4>applying replacements and mappings to several columns in the merged_df and merged_df_test data frames in order to encode ordinal and categorical variables, respectively.</h4>"""

# List of columns to be processed
columns = ['Platform_Location', 'Gender', 'Customer_Type', 'Type_Travel', 'Travel_Class']

# Loop through columns and apply replacements in merged_df(_test)
for col in columns:
    # For merged_df
    merged_df = merged_df.assign(**{
        col: merged_df[col].replace(['Very Convenient', 'Needs Improvement', 'Manageable', 'Inconvenient', 'Convenient', 'Very Inconvenient'],
                                  [5, 2, 3, 1, 4, 0])
    })

    # For merged_df_test
    merged_df_test = merged_df_test.assign(**{
        col: merged_df_test[col].replace(['Very Convenient', 'Needs Improvement', 'Manageable', 'Inconvenient', 'Convenient', 'Very Inconvenient'],
                                       [5, 2, 3, 1, 4, 0])
    })

# Define mapping for the remaining columns
mapping = {'Gender': {'Female': 0, 'Male': 1},
          'Seat_Class': {'Green Car': 0, 'Ordinary': 1},
          'Customer_Type': {'Loyal Customer': 0, 'Disloyal Customer': 1},
          'Type_Travel': {'Business travel': 0, 'Personal Travel': 1},
          'Travel_Class': {'Business': 0, 'Eco': 1}}

# Loop through columns and apply mappings in merged_df(_test)
for col, mapping_dict in mapping.items():
    # For merged_df
    merged_df = merged_df.assign(**{
        col: merged_df[col].replace(mapping_dict)
    })

    # For merged_df_test
    merged_df_test = merged_df_test.assign(**{
        col: merged_df_test[col].replace(mapping_dict)
    })

# Convert object columns to categorical
for feature in merged_df.columns:
    if merged_df[feature].dtype == 'object':
        merged_df[feature] = pd.Categorical(merged_df[feature])

for feature in merged_df_test.columns:
    if merged_df_test[feature].dtype == 'object':
        merged_df_test[feature] = pd.Categorical(merged_df_test[feature])

# Create dummy variables for all categorical columns
merged_df = pd.get_dummies(merged_df)
merged_df_test = pd.get_dummies(merged_df_test)

display(merged_df.info())
display(merged_df_test.info())

#seperating the independant and dependant variables
x = merged_df.drop(['ID', 'Overall_Experience', 'Departure_Delay_in_Mins', 'Arrival_Delay_in_Mins'], axis = 1)
y = merged_df['Overall_Experience']

x_test = merged_df_test.drop(['ID', 'Departure_Delay_in_Mins', 'Arrival_Delay_in_Mins'], axis = 1)
display(x_test.head())

"""<h4> Performing z-score normalization on the numerical columns of the data frames x and x_test and assigns the normalized values to df_scaled and df_test, respectively, which helps to make the data comparable and suitable for modeling purposes.</h4>"""

#scaling the data sets with zscores
from scipy.stats import zscore
df_scaled = x.apply(zscore)
display(df_scaled.head())


df_test = x_test.apply(zscore)
display(df_test.head())

#performing train-test split of (80:20) on the training data
X_train, X_test, Y_train, Y_test = train_test_split(df_scaled, y, test_size= 0.20, random_state=13)

display(X_train.shape)
display(X_test.shape)
display(Y_train.shape)
display(Y_test.shape)

#initial 2 classifiers
rf = RandomForestClassifier(n_estimators=220, max_depth=20, max_features=15, random_state=100)
dTree = DecisionTreeClassifier(random_state=9, max_depth=15)

ada1 = AdaBoostClassifier(estimator=rf, n_estimators=22, learning_rate=0.1, random_state=88)
ada2 = AdaBoostClassifier(estimator=dTree, n_estimators=20, learning_rate=0.1, random_state=88)
bgg = BaggingClassifier(random_state=5)
bgg1 = BaggingClassifier(estimator=dTree, n_estimators=50, random_state=500, max_features=10)
dTree1 = DecisionTreeClassifier(random_state=9, max_depth=15, min_samples_split=75, min_samples_leaf=15)
gb = GaussianNB()
grad = GradientBoostingClassifier(random_state=3000, learning_rate=0.1, n_estimators=100)
lr = LogisticRegression(random_state=100, penalty='l2', tol=0.0001)
lr1 = LogisticRegression(random_state=70, penalty='l1', solver='saga', tol=0.0001)
rf1 = RandomForestClassifier(n_estimators=100, random_state=100)

model = [ada1, ada2, gb, lr, lr1, bgg, bgg1, grad, dTree, dTree1, rf, rf1]

for i in model: #accuracy on train set from training data
    i.fit(X_train , Y_train)
    y_predict = i.predict(X_train)
    display(accuracy_score(Y_train, y_predict))

for i in model:  #accuracy on test set from training data
    i.fit(X_train , Y_train)
    y_predict_test = i.predict(X_test)
    display(accuracy_score(Y_test, y_predict_test))

df_test1 = merged_df_test.copy()
rf.fit(X_train, Y_train)

prob = rf.predict_proba(df_test)
pred = rf.predict(df_test)

for i in range(0,35602):
    if(prob[i, 1] > 0.55):
        pred[i] = 1

df_test1['Overall_Experience'] = pred
sample = df_test1[['ID', 'Overall_Experience']]
sample.to_csv('sample.csv')

ada1 = AdaBoostClassifier(estimator=rf, n_estimators=32, learning_rate=0.1, random_state= 88)
ada1.fit(X_train, Y_train)
prob = ada1.predict_proba(X_test)
y_predict_test = ada1.predict(X_test)

for i in range(0,18876):
    if(prob[i, 1] > 0.60):
        y_predict_test[i] = 1

y_predict = ada1.predict(X_train)
display(accuracy_score(Y_train, y_predict))

y_predict_test = ada1.predict(X_test)
display(accuracy_score(Y_test, y_predict_test))

print(classification_report(Y_test, y_predict_test))

"""## Key findings

The output displays the accuracy scores of 12 different models on the test set.

1. The highest accuracy score is 0.950, which was achieved by two models: AdaBoost with default parameters (ada1) and AdaBoost with a learning rate of 0.1 (ada2).

2. The lowest accuracy score is 0.815, which was achieved by the Gradient Boosting model (gb).

3. The Logistic Regression model (lr) and Random Forest model with default parameters (rf) both achieved an accuracy score of 0.930.

4. while the other models fell somewhere in between. Overall, the AdaBoost models appear to perform the best on this particular dataset, with an accuracy of 95%
"""

# create output csv file with predictions for each customer
df_testada = merged_df_test.copy()
pred = ada1.predict(df_test)
df_testada['Overall_Experience'] = pred
sample = df_testada[['ID', 'Overall_Experience']]

sample.to_csv('customer_pred.csv')